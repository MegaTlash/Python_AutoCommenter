{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bac73c1-206f-42e3-907f-29b06e0a08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b95cf844-56d0-4cc7-af38-edb6e3c0d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de4bff-e6f6-40ae-9aa3-66a4aa8ed12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c28fd04c-cf75-4c9f-aee9-f87e5aaa4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fa1686-a705-4fea-ad3d-fa463e624bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "981c6c18-7825-4079-9062-d567f83d86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b345ef74-f27b-424c-994d-fa80862b18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "790846bb-82f4-4ffc-9c55-6b72e6a1369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"dataset_modi.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fa64840-e90e-4adb-ac8d-9bc09df5361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8f0144a-7df4-4080-85db-de56b4bbc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2aa3fe1-daa4-41d9-a849-4f9030a61fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "516e67b0-1a02-4528-9ff0-8e465fca2444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['imports', 'instruction', 'output', 'input', 'text'],\n",
      "    num_rows: 268\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42434321-22d4-43e8-962e-c03eae4e6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2df5fc24-bfe4-4054-9e74-c820fbb86d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4080. Max memory = 15.992 GB.\n",
      "11.561 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62bb6ec7-50a6-4e3a-a7f7-775a9a942595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 268 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 30\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 02:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.570500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.507100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23c213f7-3d14-4207-9e0b-cc5ddb6bbd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.9923 seconds used for training.\n",
      "2.37 minutes used for training.\n",
      "Peak reserved memory = 11.561 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 72.292 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "668ea7c1-79c0-40bb-901e-02f096c2920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite detailed and informative comments for the Python function provided. The comments should include a high-level overview of the function\\'s purpose, detailed descriptions of each parameter and what they represent, an explanation of the function\\'s return values, and a line-by-line breakdown of what each part of the code does. The goal is to make the function\\'s operation clear and understandable for someone who may be unfamiliar with the code.\\n\\n### Input:\\ndef format_header_param_html5(name, value):\\n    #--\\n    if isinstance(value, six.binary_type):\\n        value = value.decode(\\'utf-8\\')\\n    value = _replace_multiple(value, _HTML5_REPLACEMENTS)\\n    return u\\'%s=\"%s\"\\' % (name, value)\\n\\n### Response:\\ndef format_header_param_html5(name, value):\\n    \"\"\"\\n    Format a header parameter for HTML5.\\n\\n    :param name:\\n        The name of the header parameter.\\n    :type name: str\\n    :param value:\\n        The value of the header parameter.\\n    :type value: str\\n\\n    :return']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Write detailed and informative comments for the Python function provided. The comments should include a high-level overview of the function's purpose, detailed descriptions of each parameter and what they represent, an explanation of the function's return values, and a line-by-line breakdown of what each part of the code does. The goal is to make the function's operation clear and understandable for someone who may be unfamiliar with the code.\", # instruction\n",
    "        \"def do_file(functions, fname, in_place=False):\\n  #--\\n  if in_place:\\n        nfname = fname\\n    else:\\n        nfname = fname[:-3] + '_modified.py'\\n    with open(nfname, 'w', encoding='utf-8') as mod:\\n        last_lineno = -1\\n        last_col = 0\\n        for func in functions:\\n            source = StringIO('\\\\n' + func)\\n            tokgen = tokenize.generate_tokens(source.readline)\\n            prev_toktype = tokenize.INDENT\\n            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\\n                if 0:\\n                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\\n                if slineno > last_lineno:\\n                    last_col = 0\\n                if scol > last_col:\\n                    mod.write(' ' * (scol - last_col))\\n                if toktype == token.STRING and prev_toktype == token.INDENT:\\n                    mod.write('#--')\\n                elif toktype == tokenize.COMMENT:\\n                    mod.write('##\\\\n')\\n                else:\\n                    mod.write(ttext)\\n                prev_toktype = toktype\\n                last_col = ecol\\n                last_lineno = elineno\\n    return nfname\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "baddfd2b-b3ea-4f2e-8915-4fe91b2916cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write detailed and informative comments for the Python function provided. The comments should include a high-level overview of the function's purpose, detailed descriptions of each parameter and what they represent, an explanation of the function's return values, and a line-by-line breakdown of what each part of the code does. The goal is to make the function's operation clear and understandable for someone who may be unfamiliar with the code.\n",
      "\n",
      "### Input:\n",
      "def do_file(functions, fname, in_place=False):\n",
      "  #--\n",
      "  if in_place:\n",
      "        nfname = fname\n",
      "    else:\n",
      "        nfname = fname[:-3] + '_modified.py'\n",
      "    with open(nfname, 'w', encoding='utf-8') as mod:\n",
      "        last_lineno = -1\n",
      "        last_col = 0\n",
      "        for func in functions:\n",
      "            source = StringIO('\\n' + func)\n",
      "            tokgen = tokenize.generate_tokens(source.readline)\n",
      "            prev_toktype = tokenize.INDENT\n",
      "            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\n",
      "                if 0:\n",
      "                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\n",
      "                if slineno > last_lineno:\n",
      "                    last_col = 0\n",
      "                if scol > last_col:\n",
      "                    mod.write(''* (scol - last_col))\n",
      "                if toktype == token.STRING and prev_toktype == token.INDENT:\n",
      "                    mod.write('#--')\n",
      "                elif toktype == tokenize.COMMENT:\n",
      "                    mod.write('##\\n')\n",
      "                else:\n",
      "                    mod.write(ttext)\n",
      "                prev_toktype = toktype\n",
      "                last_col = ecol\n",
      "                last_lineno = elineno\n",
      "    return nfname\n",
      "\n",
      "### Response:\n",
      "def do_file(functions, fname, in_place=False):\n",
      "    \"\"\"\n",
      "    Write modified functions to a file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    functions : list of str\n",
      "        List of function definitions.\n",
      "    fname : str\n",
      "        File name to write to.\n",
      "    in_place : bool, optional\n",
      "        If True, write to the same file.  Otherwise, write to a new file with\n",
      "        the same name but with ``_modified`` appended.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    fname : str\n",
      "        File name written to.\n",
      "\n",
      "    \"\"\"\n",
      "    if in_place:\n",
      "        nfname = fname\n",
      "    else:\n",
      "        nfname = fname[:-3] + '_modified.py'\n",
      "    with open(nfname, 'w', encoding='utf-8') as mod:\n",
      "        last_lineno = -1\n",
      "        last_col = 0\n",
      "        for func in functions:\n",
      "            source = StringIO('\\n' + func)\n",
      "            tokgen = tokenize.generate_tokens(source.readline)\n",
      "            prev_toktype = tokenize.INDENT\n",
      "            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\n",
      "                if 0:\n",
      "                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\n",
      "                if slineno > last_lineno:\n",
      "                    last_col = 0\n",
      "                if scol > last_col:\n",
      "                    mod.write(' * (scol - last_col))\n",
      "                if toktype == token.STRING and prev_toktype == token.INDENT:\n",
      "                    mod.write('#--')\n",
      "                elif toktype == tokenize.COMMENT:\n",
      "                    mod.write('##\\n')\n",
      "                else:\n",
      "                    mod.write(ttext)\n",
      "                prev_toktype = toktype\n",
      "                last_col = ecol\n",
      "                last_lineno = elineno\n",
      "    return nfname<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Write detailed and informative comments for the Python function provided. The comments should include a high-level overview of the function's purpose, detailed descriptions of each parameter and what they represent, an explanation of the function's return values, and a line-by-line breakdown of what each part of the code does. The goal is to make the function's operation clear and understandable for someone who may be unfamiliar with the code.\", # instruction\n",
    "        \"def do_file(functions, fname, in_place=False):\\n  #--\\n  if in_place:\\n        nfname = fname\\n    else:\\n        nfname = fname[:-3] + '_modified.py'\\n    with open(nfname, 'w', encoding='utf-8') as mod:\\n        last_lineno = -1\\n        last_col = 0\\n        for func in functions:\\n            source = StringIO('\\\\n' + func)\\n            tokgen = tokenize.generate_tokens(source.readline)\\n            prev_toktype = tokenize.INDENT\\n            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\\n                if 0:\\n                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\\n                if slineno > last_lineno:\\n                    last_col = 0\\n                if scol > last_col:\\n                    mod.write(' ' * (scol - last_col))\\n                if toktype == token.STRING and prev_toktype == token.INDENT:\\n                    mod.write('#--')\\n                elif toktype == tokenize.COMMENT:\\n                    mod.write('##\\\\n')\\n                else:\\n                    mod.write(ttext)\\n                prev_toktype = toktype\\n                last_col = ecol\\n                last_lineno = elineno\\n    return nfname\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea336af8-d534-4434-b1a3-a3f105bab3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a585f40e-bca0-44e3-a9b1-d120f0e7bd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0f3f1ffd6346d68d4feb8e10ece88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f0f9867-ebfb-4bee-a134-97c42a31227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"lora_model\") # Local saving\n",
    "#tokenizer.save_pretrained(\"lora_model\")\n",
    "#model.push_to_hub(\"MegaTrash/Llama3_Python_Commenter\", token = \"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\") # Online saving\n",
    "#tokenizer.push_to_hub(\"MegaTrash/Llama3_Python_Commenter\", token = \"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\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d25fe8f-d7ee-429a-91c5-aff04e46e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5575c943-846c-47b7-8ffc-2897c5ed5050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Write detailed and informative comments for the Python function provided. The comments should include a high-level overview of the function's purpose, detailed descriptions of each parameter and what they represent, an explanation of the function's return values, and a line-by-line breakdown of what each part of the code does. The goal is to make the function's operation clear and understandable for someone who may be unfamiliar with the code.\", # instruction\n",
    "        \"def do_file(functions, fname, in_place=False):\\n  #--\\n  if in_place:\\n        nfname = fname\\n    else:\\n        nfname = fname[:-3] + '_modified.py'\\n    with open(nfname, 'w', encoding='utf-8') as mod:\\n        last_lineno = -1\\n        last_col = 0\\n        for func in functions:\\n            source = StringIO('\\\\n' + func)\\n            tokgen = tokenize.generate_tokens(source.readline)\\n            prev_toktype = tokenize.INDENT\\n            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\\n                if 0:\\n                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\\n                if slineno > last_lineno:\\n                    last_col = 0\\n                if scol > last_col:\\n                    mod.write(' ' * (scol - last_col))\\n                if toktype == token.STRING and prev_toktype == token.INDENT:\\n                    mod.write('#--')\\n                elif toktype == tokenize.COMMENT:\\n                    mod.write('##\\\\n')\\n                else:\\n                    mod.write(ttext)\\n                prev_toktype = toktype\\n                last_col = ecol\\n                last_lineno = elineno\\n    return nfname\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
    "m_out = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "916126c1-ac2d-4290-a7c2-a35be99e8f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write detailed and informative comments for the Python function provided. The comments should include a high-level overview of the function's purpose, detailed descriptions of each parameter and what they represent, an explanation of the function's return values, and a line-by-line breakdown of what each part of the code does. The goal is to make the function's operation clear and understandable for someone who may be unfamiliar with the code.\n",
      "\n",
      "### Input:\n",
      "def do_file(functions, fname, in_place=False):\n",
      "  #--\n",
      "  if in_place:\n",
      "        nfname = fname\n",
      "    else:\n",
      "        nfname = fname[:-3] + '_modified.py'\n",
      "    with open(nfname, 'w', encoding='utf-8') as mod:\n",
      "        last_lineno = -1\n",
      "        last_col = 0\n",
      "        for func in functions:\n",
      "            source = StringIO('\\n' + func)\n",
      "            tokgen = tokenize.generate_tokens(source.readline)\n",
      "            prev_toktype = tokenize.INDENT\n",
      "            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\n",
      "                if 0:\n",
      "                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\n",
      "                if slineno > last_lineno:\n",
      "                    last_col = 0\n",
      "                if scol > last_col:\n",
      "                    mod.write(''* (scol - last_col))\n",
      "                if toktype == token.STRING and prev_toktype == token.INDENT:\n",
      "                    mod.write('#--')\n",
      "                elif toktype == tokenize.COMMENT:\n",
      "                    mod.write('##\\n')\n",
      "                else:\n",
      "                    mod.write(ttext)\n",
      "                prev_toktype = toktype\n",
      "                last_col = ecol\n",
      "                last_lineno = elineno\n",
      "    return nfname\n",
      "\n",
      "### Response:\n",
      "def do_file(functions, fname, in_place=False):\n",
      "    \"\"\"\n",
      "    Write modified functions to a file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    functions : list of str\n",
      "        List of function definitions.\n",
      "    fname : str\n",
      "        File name to write to.\n",
      "    in_place : bool, optional\n",
      "        If True, write to the same file.  Otherwise, write to a new file with\n",
      "        the same name but with ``_modified`` appended.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    fname : str\n",
      "        File name written to.\n",
      "\n",
      "    \"\"\"\n",
      "    if in_place:\n",
      "        nfname = fname\n",
      "    else:\n",
      "        nfname = fname[:-3] + '_modified.py'\n",
      "    with open(nfname, 'w', encoding='utf-8') as mod:\n",
      "        last_lineno = -1\n",
      "        last_col = 0\n",
      "        for func in functions:\n",
      "            source = StringIO('\\n' + func)\n",
      "            tokgen = tokenize.generate_tokens(source.readline)\n",
      "            prev_toktype = tokenize.INDENT\n",
      "            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\n",
      "                if 0:\n",
      "                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\n",
      "                if slineno > last_lineno:\n",
      "                    last_col = 0\n",
      "                if scol > last_col:\n",
      "                    mod.write(''* (scol - last_col))\n",
      "                if toktype == token.STRING and prev_toktype == token.INDENT:\n",
      "                    mod.write('#--')\n",
      "                elif toktype == tokenize.COMMENT:\n",
      "                    mod.write('##\\n')\n",
      "                else:\n",
      "                    mod.write(ttext)\n",
      "                prev_toktype = toktype\n",
      "                last_col = ecol\n",
      "                last_lineno = elineno\n",
      "    return nfname<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(m_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb65a50c-212a-4123-85b4-8744c7b3f7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def do_file(functions, fname, in_place=False):\n",
      "    \"\"\"\n",
      "    Write modified functions to a file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    functions : list of str\n",
      "        List of function definitions.\n",
      "    fname : str\n",
      "        File name to write to.\n",
      "    in_place : bool, optional\n",
      "        If True, write to the same file.  Otherwise, write to a new file with\n",
      "        the same name but with ``_modified`` appended.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    fname : str\n",
      "        File name written to.\n",
      "\n",
      "    \"\"\"\n",
      "    if in_place:\n",
      "        nfname = fname\n",
      "    else:\n",
      "        nfname = fname[:-3] + '_modified.py'\n",
      "    with open(nfname, 'w', encoding='utf-8') as mod:\n",
      "        last_lineno = -1\n",
      "        last_col = 0\n",
      "        for func in functions:\n",
      "            source = StringIO('\\n' + func)\n",
      "            tokgen = tokenize.generate_tokens(source.readline)\n",
      "            prev_toktype = tokenize.INDENT\n",
      "            for (toktype, ttext, (slineno, scol), (elineno, ecol), ltext) in tokgen:\n",
      "                if 0:\n",
      "                    print('%10s %-14s %-20r %r' % (tokenize.tok_name.get(toktype, toktype), '%d.%d-%d.%d' % (slineno, scol, elineno, ecol), ttext, ltext))\n",
      "                if slineno > last_lineno:\n",
      "                    last_col = 0\n",
      "                if scol > last_col:\n",
      "                    mod.write(''* (scol - last_col))\n",
      "                if toktype == token.STRING and prev_toktype == token.INDENT:\n",
      "                    mod.write('#--')\n",
      "                elif toktype == tokenize.COMMENT:\n",
      "                    mod.write('##\\n')\n",
      "                else:\n",
      "                    mod.write(ttext)\n",
      "                prev_toktype = toktype\n",
      "                last_col = ecol\n",
      "                last_lineno = elineno\n",
      "    return nfname<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "res = m_out[0].split(\"### Response:\\n\", 1)\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9e6fd-a7bd-4b2c-9b3a-8fbc7fb92e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
